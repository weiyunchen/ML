{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 电影评论情感分析 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无意中看到了这个数据集，该数据集包含了豆瓣上的 28 部电影的 200 万个短评，正好拿来练一练NLP。我将使用该数据集建立NLP的模型并进行测试，本实验暂时分为以下几部分：\n",
    "- NN\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /home/v/anaconda3/lib/python3.6/site-packages (0.39)\n",
      "--2019-05-09 03:35:43--  https://codeload.github.com/weiyunchen/nlp/zip/master\n",
      "正在解析主机 codeload.github.com (codeload.github.com)... 13.250.162.133\n",
      "正在连接 codeload.github.com (codeload.github.com)|13.250.162.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 未指定 [application/zip]\n",
      "正在保存至: “master”\n",
      "\n",
      "master                  [   <=>              ]  75.41K   142KB/s    用时 0.5s    \n",
      "\n",
      "2019-05-09 03:35:45 (142 KB/s) - “master” 已保存 [77224]\n",
      "\n",
      "Archive:  master\n",
      "78a55b3ff658cd83cd1ae4abbe5fb475507097c3\n",
      "   creating: nlp-master/\n",
      "  inflating: nlp-master/faker.png    \n",
      "  inflating: nlp-master/stopwords.txt  \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install jieba\n",
    "!wget -nc \"https://codeload.github.com/weiyunchen/nlp/zip/master\"\n",
    "!unzip -o master\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "首先，我们读取并预览 CSV 数据文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>然潘</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>2</td>\n",
       "      <td>更深的白色</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>2</td>\n",
       "      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>有意识的贱民</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>2</td>\n",
       "      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>4</td>\n",
       "      <td>不老的李大爷耶</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>4</td>\n",
       "      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>ZephyrO</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>2</td>\n",
       "      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username  \\\n",
       "ID                                                                      \n",
       "0   Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘   \n",
       "1   Avengers Age of Ultron        复仇者联盟2  2017-01-22       2    更深的白色   \n",
       "2   Avengers Age of Ultron        复仇者联盟2  2017-01-22       3   有意识的贱民   \n",
       "3   Avengers Age of Ultron        复仇者联盟2  2017-01-22       4  不老的李大爷耶   \n",
       "4   Avengers Age of Ultron        复仇者联盟2  2017-01-22       5  ZephyrO   \n",
       "\n",
       "          Date  Star                                            Comment  Like  \n",
       "ID                                                                             \n",
       "0   2015-05-13     3                                      连奥创都知道整容要去韩国。  2404  \n",
       "1   2015-04-24     2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n",
       "2   2015-04-26     2   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  \n",
       "3   2015-04-23     4   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...  1045  \n",
       "4   2015-04-22     2                                  虽然从头打到尾，但是真的很无聊啊。   723  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "douban_data = pd.read_csv('DMSC.csv', index_col=0)\n",
    "douban_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑到每个人的情感阈值存在极大差距，因此将评分分成两类，三分以下为差评\n",
    "douban_data['Star']=((douban_data.Star+0.5)/3.5+1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>然潘</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>2</td>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>2</td>\n",
       "      <td>更深的白色</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>1</td>\n",
       "      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>有意识的贱民</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>1</td>\n",
       "      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>4</td>\n",
       "      <td>不老的李大爷耶</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>2</td>\n",
       "      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>ZephyrO</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>1</td>\n",
       "      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username  \\\n",
       "ID                                                                      \n",
       "0   Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘   \n",
       "1   Avengers Age of Ultron        复仇者联盟2  2017-01-22       2    更深的白色   \n",
       "2   Avengers Age of Ultron        复仇者联盟2  2017-01-22       3   有意识的贱民   \n",
       "3   Avengers Age of Ultron        复仇者联盟2  2017-01-22       4  不老的李大爷耶   \n",
       "4   Avengers Age of Ultron        复仇者联盟2  2017-01-22       5  ZephyrO   \n",
       "\n",
       "          Date  Star                                            Comment  Like  \n",
       "ID                                                                             \n",
       "0   2015-05-13     2                                      连奥创都知道整容要去韩国。  2404  \n",
       "1   2015-04-24     1   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n",
       "2   2015-04-26     1   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  \n",
       "3   2015-04-23     2   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...  1045  \n",
       "4   2015-04-22     1                                  虽然从头打到尾，但是真的很无聊啊。   723  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "douban_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据由 9 列构成，分别是：电影英文名、中文名、爬取日期、评论 ID（从 0 开始）、用户名、发表日期、评价分数、评论内容、点赞数。然后划分数据集，随机抽取近 10000 条数据，并按电影和评分均等划分。最终实际数据为 2125032 条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125032, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = douban_data.groupby(['Movie_Name_CN', 'Star']).apply(\n",
    "    lambda x: x.sample(n=int(2125056/(28*2)), replace=True, random_state=0))\n",
    "\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "`groupby` 函数对 DataFrame 按照 `'Movie_Name_CN', 'Star'` 进行分组（两列值相同为一组），参考 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> pandas.DataFrame.groupby</i>](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)，`apply` 传入一个 `lambda` 函数，对每一组调用 `sample` 函数随机抽取出 `n` 条数据，参考 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> pandas.DataFrame.sample</i>](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "接下来我们将数据划分为 80% 的训练集和 20% 的测试集，并打乱顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700025, 425007, 1700025, 425007)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "comments = sample_df.values[:, 7]\n",
    "star = sample_df.values[:, 6]\n",
    "\n",
    "x_train, x_test, y_train, y_test, = train_test_split(\n",
    "    comments, star, test_size=0.2, random_state=0)\n",
    "\n",
    "len(y_train), len(y_test), len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "本次实验在数据预处理和前面实验楼评论数据情感分析实验相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 分词处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用结巴分词，并返回分词结果和标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebfcb7a836b4a2fb44becb855020b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1700025), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.652 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16f634afdb74fb6b5ab9b47d36473ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=425007), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "# 清理非中文字符，替换不需要的字符串\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()\n",
    "\n",
    "\n",
    "# 加载停用词\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords = [line.strip('\\n') for line in f.readlines()]\n",
    "\n",
    "\n",
    "def cut(data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(data))):\n",
    "        comment = clean_str(data[index])\n",
    "        label = labels[index]\n",
    "        # 分词\n",
    "        seg_list = jieba.cut(comment, cut_all=False, HMM=True)\n",
    "        seg_list = [x.strip('\\n')\n",
    "                    for x in seg_list if x not in stopwords and len(x) > 1]\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(seg_list)\n",
    "            new_labels.append(label)\n",
    "    # 返回分词结果和对应的标签\n",
    "    return result, new_labels\n",
    "\n",
    "\n",
    "# 分别对训练数据和测试数据分词\n",
    "train_cut_result, train_labels = cut(x_train, y_train, stopwords)\n",
    "test_cut_result, test_labels = cut(x_test, y_test, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "接下来，我们尝试统计训练和测试评论数据所在的分段，然后绘制出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "可以看出，随机划分后的训练和测试评论数据位于不同分段上的数据条数还是比较均衡的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 词向量转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "任何机器学习算法都无法直接理解词语的含义，所以我们需要将自然语言处理成算法能够理解的词向量，这里我们用到 TF-IDF 模型。TF-IDF 由两部分组成：TF（Term frequency，词频），IDF（Inverse document frequency，逆文档频率）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "TF-IDF 根据词频即词语在数据中出现的次数。总的来说，就是把一段话的每个词都配之以重要性指标，某个词在这段话的出现次数多时，它就更重要。比如一段话中提到了三次「中国」，就比另一段只提到了一次的重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "此外，当这个词在所有话中出现的次数多时（比如「苹果」，「乔布斯」等不常见词），这个词就更加的重要。当这个词在所有的段落中出现次数都多时（比如「的」，「这」等常见词），这个词的重要性下降。对于每个词都计算这样的重要性并写成一个向量后，映射就完成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "scikit-learn 实现了这种方法，参考 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> TfidfVectorizer</i>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)，这是 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> CountVectorizer</i>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)（统计词频） 和 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> TfidfTransformer</i>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)（转换 TF-IDF）的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer 传入原始文本\n",
    "train_data = [' '.join(x) for x in train_cut_result]\n",
    "test_data = [' '.join(x) for x in test_cut_result]\n",
    "\n",
    "# max_features指定语料库中频率最高的词\n",
    "n_dim = 35000\n",
    "\n",
    "# 数据的TF-IDF信息计算\n",
    "# sublinear_tf=True 时生成一个近似高斯分布的特征，可以提高大概1~2个百分点\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=n_dim, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "# 对训练数据训练\n",
    "train_vec_data = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# 训练完成之后对测试数据转换\n",
    "test_vec_data = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "查看词袋模型中的前十个词语:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一一列举', '一丁点', '一丁点儿', '一万', '一万个', '一万倍', '一万只', '一万年', '一万条', '一万次']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "至此数据预处理结束，得到了训练和测试数据的 TF-IDF 矩阵，接下来将使用 TF-IDF 矩阵进行训练和测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先尝试使用多层神经网络实现豆瓣电影评论分类，并通过 PyTorch 深度学习框架完成。首先，实现需要定义一些参数，例如类别数、学习率、损失函数、迭代次数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 输出的类别为 2\n",
    "n_categories = 2\n",
    "# 学习率，请不要过大，会导致 loss 震荡\n",
    "learning_rate = 0.001\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 迭代次数\n",
    "epochs = 6\n",
    "# 每次迭代同时加载的个数\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "在 PyTorch 中对自定义数据集都需要写一个 Dataset 进行加载数据，然后在 DataLoader 中使用。所以，第一步是定义一个自己的 Dataset 类，重写 `__getitem__` 方法，获取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TxtDataset(Dataset):\n",
    "    def __init__(self, VectData, labels):\n",
    "        # 传入初始数据，特征向量和标签\n",
    "        self.VectData = VectData\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # DataLoader 会根据 index 获取数据\n",
    "        # toarray() 是因为 VectData 是一个稀疏矩阵，如果直接使用 VectData.toarray() 占用内存太大，请勿尝试\n",
    "        return self.VectData[index].toarray(), self.labels[index]-1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 线下内存足够大可以考虑增大 num_workers，并行读取数据\n",
    "# 加载训练数据集\n",
    "train_dataset = TxtDataset(train_vec_data, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=50\n",
    "                              )\n",
    "# 加载测试数据集\n",
    "test_dataset = TxtDataset(test_vec_data, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=50\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "定义一个神经网络，由线性层、激活函数、Dropout 依次组成。这里，我们仅仅定义了一个全连接网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class TxtModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TxtModel, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x.double())\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "训练过程中，选用 Adam 作为优化器，`exp_lr_scheduler` 是为了每个步长衰减学习率。最后输出损失、准确度和执行时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62e21161bf14a08bcb1e0bfac41672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0/6:\n",
      "training loss: 0.004015722215006582, time resumed 1494.8732607364655s\n",
      "testing loss: 0.003235486754182074, time resumed 167.8843846321106s, accuracy: 0.8648242564804465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7680b43ecef4877932cea84a640c059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1/6:\n",
      "training loss: 0.0027647626172974784, time resumed 1533.0914461612701s\n",
      "testing loss: 0.002698652265290571, time resumed 164.81334781646729s, accuracy: 0.8897648111548143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987a1bc4c43f4a61a718d436721f9b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2/6:\n",
      "training loss: 0.0022649255073836257, time resumed 1530.1845548152924s\n",
      "testing loss: 0.0024865395633903394, time resumed 165.37635612487793s, accuracy: 0.9011325972241193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8253d6d96474b80b9cd9d573f786861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3/6:\n",
      "training loss: 0.0017764741919218477, time resumed 1537.6156966686249s\n",
      "testing loss: 0.002411478553855628, time resumed 169.7466697692871s, accuracy: 0.9065070679307452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143505261c384d15b4e27ad46c1872e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4/6:\n",
      "training loss: 0.0016524011864852558, time resumed 1539.3705825805664s\n",
      "testing loss: 0.0023988213636887976, time resumed 170.16531586647034s, accuracy: 0.9087482891175331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c7016bb45541a2a34f67d049dc1a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5/6:\n",
      "training loss: 0.0015721819568429492, time resumed 1548.9475784301758s\n",
      "testing loss: 0.002389944676247696, time resumed 170.98718237876892s, accuracy: 0.9105769472002855\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# 定义模型和优化器\n",
    "model = TxtModel(n_dim, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 每两代衰减学习率\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer, step_size=int(epochs/2), gamma=0.1)\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    #model = nn.DataParallel(model)\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "model = model.double()\n",
    "\n",
    "# 保存准确度最高的模型\n",
    "best_model = copy.deepcopy(model)\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    exp_lr_scheduler.step()\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    st = time.time()\n",
    "    # train_dataloader 加载数据集\n",
    "    for data, label in tqdm_notebook(train_dataloader):\n",
    "        # 如果 GPU 可用，则使用 GPU\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        output = model(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # 输出损失、训练时间等\n",
    "    print('epoch {}/{}:'.format(epoch, epochs))\n",
    "    print('training loss: {}, time resumed {}s'.format(\n",
    "        loss_total/len(train_dataset), time.time()-st))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "    st = time.time()\n",
    "\n",
    "    correct = 0\n",
    "    for data, label in test_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    # 如果准确度取得最高，则保存准确度最高的模型\n",
    "    if correct/len(test_dataset) > best_accuracy:\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    print('testing loss: {}, time resumed {}s, accuracy: {}'.format(\n",
    "        loss_total/len(test_dataset), time.time()-st, correct/len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "接下来使用豆瓣 API 获取一条影评，然后对其分类:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "# 26266893 为国产科幻佳作《流浪地球》，在此以《流浪地球》的影评为例\n",
    "res = requests.get(\n",
    "    'https://api.douban.com/v2/movie/subject/26266893/comments?apikey=0df993c66c0c636e29ecbb5344252a4a')\n",
    "comments = json.loads(res.content.decode('utf-8'))['comments']\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_comments(comments):\n",
    "    test_comment = random.choice(comments)\n",
    "# 选择其中一条分类，并去除非中文字符\n",
    "    content = clean_str(test_comment['content'])\n",
    "    rating = test_comment['rating']['value']\n",
    "# 对评论分词\n",
    "    seg_list = jieba.cut(content, cut_all=False, HMM=True)\n",
    "# 去掉停用词和无意义的\n",
    "    cut_content = ' '.join([x.strip('\\n')\n",
    "                        for x in seg_list if x not in stopwords and len(x) > 1])\n",
    "\n",
    "# 转化为特征向量\n",
    "    one_test_data = vectorizer.transform([cut_content])\n",
    "\n",
    "# 转化为 pytorch 输入的 Tensor 数据，squeeze(0) 增加一个 batch 维度\n",
    "    one_test_data = torch.from_numpy(one_test_data.toarray()).unsqueeze(0)\n",
    "# 使用准确度最好的模型预测，softmax 处理输出概率，取得最大概率的下标再加 1 则为预测的标签\n",
    "    pred = torch.argmax(F.softmax(best_model(one_test_data), dim=1)) + 1\n",
    "    pred = pred.item()\n",
    "    \n",
    "    if pred==1:\n",
    "        pred='差评1'\n",
    "    else:\n",
    "        pred='好评2'\n",
    "        \n",
    "    if rating<3:\n",
    "        rat='差评1'\n",
    "    else:\n",
    "        rat='好评2'\n",
    "    print('评论内容: ',content)\n",
    "    print('关键字: ',cut_content)\n",
    "    print('观众评价: ',rat)\n",
    "    print('预测评价: ',pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面依次输出了评论内容、关键字、观众评价、预测评价。接下来，我们测试基于评论预测该观众的最终评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('观后感: ',i)\n",
    "    print(predict_comments(comments))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
